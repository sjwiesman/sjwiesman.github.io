---
layout: post
title: Counting at Scale
---

MediaMath processes terabytes of data each day for the various reports available in T1. One metric we show is the number of unique impressions for each campaign. While this is conceptually a simple problem, solving it at scale is not so straight forward. 

The canonical way of solving this problem would be for any given campaign put the uuid of each person into a set and then check its size. It is easy to understand the appeal of this approach; sets are provided in all modern standard libraries so it is easy to implement, and it is a mathematically sound way of solving the problem. 

Best of all, implementing this on top of Hadoop would be straight forward. The map step would be to place all the items into sets and the reduce phase would be to combine them and then output the sets size. 

##I thought you said this was going to be difficult?

The problem with the naive solution is that eventually everything will need to be piped to a single reducer. What happens if the sets are so large that combining them takes a prohibitively long amount of time? What happens if the sets are so large they can't all fit on one machine? This might not seem like a common problem but imagine we are counting the unique number of strings in a text file. 

An empty string on the JVM requires 40 bytes and a string with 5 characters, the average length of a word in the English language, requires 48 bytes. This means that for at least half of all English words we are using more memory for overhead than to store the actual characters! Even if we are only looking at numeric id's, a Long is 64 Bytes times the number of unique impressions for a given campaign times the total number of campaigns being run. It adds up quickly.  To reduce memory usage, we can heavily optimize an algorithm towards doing one thing, counting uniques. 

##HyperLogLog to the rescueHyperLogLog, instead of storing each item, makes use of the random uniform distribution of a hash function in order to estimate the number of items seen given a specific phenomenon. If that made sense to you on first pass, then you are a lot smarter than I am and can stop reading. If not, we will break it apart to understand how it works. Hash functions are generally understood as a transformation of an object from a larger space to a smaller space. Most developers conceptual understanding of of a hash function is that the hash of an object will be a random looking number which only has a very small chance of being the same as the hash of any other object. Another way of viewing a hash function is that it returns a uniformly distributed bit stream or a list of 1’s and 0’s where the probability of each bit being a specific value is independent of the value of every other bit. As stated above, we are looking for a specific phenomenon; the number of consecutive 0’s at the beginning of the stream. Because each bit is independent of every other the odds that the first bit is 0 is $$\frac{1}{2}$$, the odds that the nth bit is 0 is $$\frac{1}{2}$$, and the odds that the first consecutive n bits are 0 is $$\frac{1}{2}^n$$. To be clear, we are interested in the probability of different patterns not specific numbers. The odds of a function producing the hashes 00101101 and 00010110 are the same but the probability of seeing the pattern 00xxxxx (two zeros followed by some numbers) is twice as likely as seeing the pattern 000xxxx (three zeros followed by some numbers).  This means that if the largest number of consecutive 0’s seen is 32 we know that the odds of that happening are 1/4294967296 and we can expect that we have seen 4,294,967,296 distinct items. If we were to stop here, then we would have the algorithm LogLog. LogLog provides a very small memory footprint. The number 32 is the same as the binary number 100000b which is only 6 bits and with those 6 bits we can count up to $$2^64$$ items. LogLog’s name comes from the fact that it only requires Log(Log(n)) bits to count up to n. The problem with LogLog is that it is incredibly susceptible to outliers. If the first item seen hashes to 000000000001 then LogLog will tell us it has seen 2048 distinct items when it has only seen one. HyperLogLog is an evolution of LogLog that minimizes the influence of outliers in a data set. To increase accuracy HyperLogLog maintains a group of buckets. For each item it sees it will calculate its hash and take the first k bits of the hash to use that to pick a bucket. It will then count the number of 0-bits after that point and insert the number found if it is greater than the number already in that bucket. $$
hash(n) = \underbrace{00100101}_\text{k=8}\underbrace{\overbrace{0000000}^\text{7 consecutive 0's}1110101}_\text{search area}
$$

So given this hash function and $$k = 8$$ we would store the number 7 into bucket 37 if that is greater than the number already in that bucket. 
By storing multiple 0-bit sequence lengths, even if HyperLogLog does find one or two outliers it will have many more numbers which accurately represent the number of items seen. The buckets are combined by taking their harmonic mean which throws out outliers. The authors of the paper also present several constants for range and bias correction although that is outside the scope of this post.    While its memory footprint is much smaller than a basic set, it is still just as easy to parallelize across many machines. Each machine first calculates its own group of HyperLogLog buckets from the content that it has locally. These individual results are then combined by takeing the max value of each corresponding bucket. Finally, the resulting group of buckets can be combined to generate a final estimate.    p.s. This post contains 301 distinct words